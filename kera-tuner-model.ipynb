{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CommonLit - Evaluate Student Summaries\n\nhttps://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview","metadata":{}},{"cell_type":"markdown","source":"## Context\nSummary writing is an important skill for learners of all ages. Summarization enhances reading comprehension, particularly among second language learners and students with learning disabilities. Summary writing also promotes critical thinking, and it’s one of the most effective ways to improve writing abilities. However, students rarely have enough opportunities to practice this skill, as evaluating and providing feedback on summaries can be a time-intensive process for teachers. Innovative technology like large language models (LLMs) could help change this, as teachers could employ these solutions to assess summaries quickly.\n\nThere have been advancements in the automated evaluation of student writing, including automated scoring for argumentative or narrative writing. However, these existing techniques don't translate well to summary writing. Evaluating summaries introduces an added layer of complexity, where models must consider both the student writing and a single, longer source text. Although there are a handful of current techniques for summary evaluation, these models have often focused on assessing automatically-generated summaries rather than real student writing, as there has historically been a lack of these types of datasets.\n\nCompetition host CommonLit is a nonprofit education technology organization. CommonLit is dedicated to ensuring that all students, especially students in Title I schools, graduate with the reading, writing, communication, and problem-solving skills they need to be successful in college and beyond. The Learning Agency Lab, Vanderbilt University, and Georgia State University join CommonLit in this mission.\n\nAs a result of your help to develop summary scoring algorithms, teachers and students alike will gain a valuable tool that promotes this fundamental skill. Students will have more opportunities to practice summarization, while simultaneously improving their reading comprehension, critical thinking, and writing abilities.","metadata":{}},{"cell_type":"markdown","source":"## Imports and Package Installation","metadata":{}},{"cell_type":"code","source":"# !pip install nltk\n# !pip install seaborn\n# !pip install keras_tuner\n# !pip install kaggle\n# !pip install transformers\n# # !pip install \"/kaggle/input/autocorrect/autocorrect-2.6.1.tar\"\n# # !pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\"\n# !pip install datasets\n# !pip install spacy\n# !python -m spacy download en_core_web_sm\n# !pip install textstat\n# ! pip install nltk\n# ! pip install gensim\n# ! pip install pyLDAvis\n# ! pip install seaborn\n# ! pip install textblob\n# ! pip install textstat\n# ! pip install spacy\n# ! python -m spacy download en_core_web_sm > /dev/null 2>&1\n# ! pip install pyspellchecker","metadata":{"execution":{"iopub.status.busy":"2023-10-12T02:25:04.616263Z","iopub.execute_input":"2023-10-12T02:25:04.616699Z","iopub.status.idle":"2023-10-12T02:25:04.647954Z","shell.execute_reply.started":"2023-10-12T02:25:04.616665Z","shell.execute_reply":"2023-10-12T02:25:04.646843Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import subprocess\n\ndef install_package(package_name, extra_command=None):\n    \"\"\"\n    Install a Python package using pip and return 'Success' or 'Fail' message.\n    \n    Args:\n        package_name (str): The name of the package to install.\n        extra_command (str, optional): Extra commands to add to the pip install command.\n    \n    Returns:\n        str: 'Success' if the package was successfully installed, 'Fail' otherwise.\n    \"\"\"\n    try:\n        # Construct the pip install command\n        if extra_command:\n            cmd = f\"pip install {package_name} {extra_command}\"\n        else:\n            cmd = f\"pip install {package_name}\"\n        \n        # Execute the command and capture the output\n        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True)\n        \n        # Check if the installation was successful\n        if result.returncode == 0:\n            return 'Success'\n        else:\n            return 'Fail'\n    except Exception as e:\n        return 'Fail'\n\n# List of packages to install\npackages = [\n    'textstat',\n    'nltk',\n    'gensim',\n    'pyLDAvis',\n    'seaborn',\n    'textblob',\n    'spacy',\n    'pyspellchecker'\n]\n\n# Install the packages and print the status\nfor package in packages:\n    print(f\"Installing {package}: {install_package(package)}\")\n\n# For the spacy model download\nprint(f\"Downloading en_core_web_sm: {install_package('en_core_web_sm', extra_command='-m spacy download')}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T02:25:07.527378Z","iopub.execute_input":"2023-10-12T02:25:07.527899Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Installing textstat: Success\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport logging\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Natural Language Processing\nimport nltk\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk import pos_tag, ne_chunk\nfrom textblob import TextBlob\n# from textstat import flesch_reading_ease, smog_index\nimport spacy\nfrom collections import Counter\nfrom gensim import corpora, models\nimport pyLDAvis.gensim as gen\nimport pyLDAvis\nimport re\n\n# Machine Learning & Data Preprocessing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Deep Learning\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Gensim\nfrom gensim.models import Word2Vec, KeyedVectors\n\n# Progress bar\nfrom tqdm import tqdm\n\n# Keras Tuner\nimport keras_tuner as kt\nfrom keras_tuner.tuners import RandomSearch\n\n# Setting logging levels and environment variables\ntf.get_logger().setLevel(logging.ERROR)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# Downloading necessary NLTK data\nnltk.download('punkt')\nnltk.download('stopwords')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T23:20:26.513916Z","iopub.execute_input":"2023-10-11T23:20:26.514269Z","iopub.status.idle":"2023-10-11T23:20:26.526386Z","shell.execute_reply.started":"2023-10-11T23:20:26.514240Z","shell.execute_reply":"2023-10-11T23:20:26.525372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n\ntf.get_logger().setLevel('ERROR')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T23:11:45.471673Z","iopub.execute_input":"2023-10-11T23:11:45.472404Z","iopub.status.idle":"2023-10-11T23:11:45.478001Z","shell.execute_reply.started":"2023-10-11T23:11:45.472377Z","shell.execute_reply":"2023-10-11T23:11:45.477053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"INPUT_DIR = '/kaggle/input/commonlit-evaluate-student-summaries/'","metadata":{"execution":{"iopub.status.busy":"2023-10-11T23:11:45.479619Z","iopub.execute_input":"2023-10-11T23:11:45.480290Z","iopub.status.idle":"2023-10-11T23:11:45.489417Z","shell.execute_reply.started":"2023-10-11T23:11:45.480258Z","shell.execute_reply":"2023-10-11T23:11:45.488528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_train = pd.read_csv(INPUT_DIR + '/summaries_train.csv')\nprompts_train = pd.read_csv(INPUT_DIR + '/prompts_train.csv')\nprint('length of summaries_train:', len(summaries_train))\nprint('length of prompts_train:',len(prompts_train),'\\n')\n\nsummaries_test = pd.read_csv(INPUT_DIR + '/summaries_test.csv')\nprompts_test = pd.read_csv(INPUT_DIR + '/prompts_test.csv')\nsample_submission = pd.read_csv(INPUT_DIR + '/sample_submission.csv')\nprint('length of summaries_test:', len(summaries_test))\nprint('length of prompts_test:',len(prompts_test))\nprint('length of sample_submission:',len(sample_submission))","metadata":{"execution":{"iopub.status.busy":"2023-10-11T23:11:45.490527Z","iopub.execute_input":"2023-10-11T23:11:45.491660Z","iopub.status.idle":"2023-10-11T23:11:45.560472Z","shell.execute_reply.started":"2023-10-11T23:11:45.491629Z","shell.execute_reply":"2023-10-11T23:11:45.559493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean Text ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport operator\nfrom spellchecker import SpellChecker\nfrom tqdm import tqdm  # Import tqdm\n\ndef clean_text(df, glove_path, paragram_path, wiki_news_path, col_name = 'text'):\n    \"\"\"\n    Preprocesses text for both training and testing datasets. \n    Includes loading embeddings, building vocabularies, cleaning text among other things.\n    \n    :param summaries_train: DataFrame with the training data\n    :param summaries_test: DataFrame with the testing data\n    :param glove_path: path to the GloVe embedding\n    :param paragram_path: path to the Paragram embedding\n    :param wiki_news_path: path to the Wiki News embedding\n    \n    :return: Preprocessed DataFrame and list of out-of-vocab words\n    \"\"\"\n    print(\"Starting text cleaning process.\")\n    \n    def load_embed(file):\n        \"\"\"\n        Load the embeddings from a file.\n        \"\"\"\n        print(f\"Loading embeddings from {file}\")\n        \n        def get_coefs(word, *arr): \n            return word, np.asarray(arr, dtype='float32')\n        \n        if file == wiki_news_path:\n            embeddings_index = dict(get_coefs(*o.split(\" \")) for o in tqdm(open(file), \"Reading Embedding File\") if len(o)>100)\n        else:\n            embeddings_index = dict(get_coefs(*o.split(\" \")) for o in tqdm(open(file, encoding='latin'), \"Reading Embedding File\"))\n        \n        print(f\"Loaded embeddings from {file}\")\n        return embeddings_index\n\n    # Load embeddings\n    print(\"Loading all embeddings.\")\n    embed_glove = load_embed(glove_path)\n    embed_paragram = load_embed(paragram_path)\n    embed_fasttext = load_embed(wiki_news_path)\n    print(\"All embeddings loaded.\")\n    \n    def build_vocab(texts):\n        \"\"\"\n        Build a vocabulary from a given list of texts.\n        \"\"\"\n        print(\"Building vocabulary.\")\n        sentences = texts.apply(lambda x: x.split()).values\n        vocab = {}\n        for sentence in tqdm(sentences, \"Populating Vocabulary\"):\n            for word in sentence:\n                try:\n                    vocab[word] += 1\n                except KeyError:\n                    vocab[word] = 1\n        print(\"Vocabulary built.\")\n        return vocab\n\n    def check_coverage(vocab, embeddings_index):\n        \"\"\"\n        Check which words in the vocabulary are covered by the embeddings.\n        \"\"\"\n        print(\"Checking coverage.\")\n        known_words = {}\n        unknown_words = {}\n        nb_known_words = 0\n        nb_unknown_words = 0\n        for word in tqdm(vocab.keys(), \"Checking Words\"):\n            try:\n                known_words[word] = embeddings_index[word]\n                nb_known_words += vocab[word]\n            except:\n                unknown_words[word] = vocab[word]\n                nb_unknown_words += vocab[word]\n                pass\n        unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n        print(\"Coverage checked.\")\n        return unknown_words\n\n    # Build and check vocab for train and test datasets\n    print(\"Processing train and test datasets.\")\n    \n    vocab_train = build_vocab(df[col_name])\n    \n    oov_glove_train = check_coverage(vocab_train, embed_glove)\n    oov_paragram_train = check_coverage(vocab_train, embed_paragram)\n    oov_fasttext_train = check_coverage(vocab_train, embed_fasttext)\n  \n    print(\"Processed train and test datasets.\")\n    \n    # Lowercase all texts\n    df['lowered_question'] = df[col_name].apply(lambda x: x.lower())\n    \n    train_vocab_low = build_vocab(df['lowered_question'])\n    \n    oov_glove_train = check_coverage(train_vocab_low, embed_glove)\n    oov_paragram_train = check_coverage(train_vocab_low, embed_paragram)\n    oov_fasttext_train = check_coverage(train_vocab_low, embed_fasttext)\n \n    \n    def add_lower(embedding, vocab):\n        count = 0\n        for word in vocab:\n            if word in embedding and word.lower() not in embedding:  \n                embedding[word.lower()] = embedding[word]\n                count += 1\n    \n    add_lower(embed_glove, train_vocab_low)\n    add_lower(embed_paragram, train_vocab_low)\n    add_lower(embed_fasttext, train_vocab_low)\n   \n    \n    # Handle contractions\n    contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\",\n                           \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n                           \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n                           \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n                           \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                           \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n                           \"you're\": \"you are\", \"you've\": \"you have\" }\n    \n    \n    def clean_contractions(text, mapping):\n        \"\"\"\n        Replace contractions in the text based on a given mapping.\n        \n        :param text: The original text\n        :param mapping: Dictionary containing contractions mapping\n        \n        :return: Text with contractions replaced\n        \"\"\"\n        \n        specials = [\"’\", \"‘\", \"´\", \"`\"]\n        \n        for s in specials:\n            text = text.replace(s, \"'\")\n        text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n        return text\n\n    # Apply contraction cleaning to train and test datasets\n    df['cleaned_text'] = df['lowered_question'].apply(lambda x: clean_contractions(x, contraction_mapping))\n    \n    # Rebuild and check vocab after cleaning contractions\n    vocab_train_clean = build_vocab(df['cleaned_text'])\n    \n    oov_glove_train = check_coverage(vocab_train_clean, embed_glove)\n    oov_paragram_train = check_coverage(vocab_train_clean, embed_paragram)\n    oov_fasttext_train = check_coverage(vocab_train_clean, embed_fasttext)\n    \n     # Add your additional code for punctuations, special characters and spelling correction here\n        \n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    \n    punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\",\n                     \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', \n                     'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n\n    def clean_special_chars(text, punct, mapping):\n        for p in mapping:\n            text = text.replace(p, mapping[p])\n        for p in punct:\n            text = text.replace(p, f' {p} ')\n        specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  \n        for s in specials:\n            text = text.replace(s, specials[s])\n        return text\n\n    df['treated_question'] = df['cleaned_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n\n    # Spell correction\n    misspelled_words = [word for word, count in oov_fasttext_train]\n\n    def spell_check_list_of_words(word_list):\n        spell = SpellChecker()\n        corrected_dict = {}\n        for word in word_list:\n            corrected_word = spell.correction(word)\n            corrected_dict[word] = corrected_word\n        return corrected_dict\n\n    mispell_dict_train = spell_check_list_of_words(misspelled_words)\n\n    def correct_spelling(x, dic):\n        if not dic:\n            return x\n        pattern = r'\\b(' + '|'.join(re.escape(key) for key in dic.keys()) + r')\\b'\n        return re.sub(pattern, lambda m: dic[m.group(0)], x, flags=re.IGNORECASE)\n\n    df['treated_question'] = df['treated_question'].apply(lambda x: correct_spelling(x, mispell_dict_train))\n    \n    # Rebuild and check vocab after cleaning contractions\n    vocab_train_clean = build_vocab(df['treated_question'])\n    \n    oov_glove_train = check_coverage(vocab_train_clean, embed_glove)\n    oov_paragram_train = check_coverage(vocab_train_clean, embed_paragram)\n    oov_fasttext_train = check_coverage(vocab_train_clean, embed_fasttext)\n\n\n    return df, oov_glove_train, oov_paragram_train, oov_fasttext_train","metadata":{"execution":{"iopub.status.busy":"2023-10-11T23:20:31.848853Z","iopub.execute_input":"2023-10-11T23:20:31.849328Z","iopub.status.idle":"2023-10-11T23:20:31.883230Z","shell.execute_reply.started":"2023-10-11T23:20:31.849290Z","shell.execute_reply":"2023-10-11T23:20:31.881566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glove = '/kaggle/input/embedding/glove.6B/glove.6B.300d.txt'\nparagram =  '/kaggle/input/paragram-300-sl999/paragram_300_sl999.txt'\nwiki_news = '/kaggle/input/embedding/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec'","metadata":{"execution":{"iopub.status.busy":"2023-10-11T23:20:32.231917Z","iopub.execute_input":"2023-10-11T23:20:32.232203Z","iopub.status.idle":"2023-10-11T23:20:32.236726Z","shell.execute_reply.started":"2023-10-11T23:20:32.232178Z","shell.execute_reply":"2023-10-11T23:20:32.235651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_train, oov_glove_train, oov_paragram_train, oov_fasttext_train = clean_text(summaries_train, glove, paragram, wiki_news, col_name = 'text')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_test, _, _, _ = clean_text(summaries_test, glove, paragram, wiki_news, col_name = 'text')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts_train, _, _, _ = clean_text(prompts_train, glove, paragram, wiki_news, col_name = 'prompt_text')","metadata":{"execution":{"iopub.status.busy":"2023-10-11T23:20:35.672157Z","iopub.execute_input":"2023-10-11T23:20:35.672527Z","iopub.status.idle":"2023-10-11T23:22:15.406215Z","shell.execute_reply.started":"2023-10-11T23:20:35.672473Z","shell.execute_reply":"2023-10-11T23:22:15.404962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts_test, _, _, _ = clean_text(prompts_test, glove, paragram, wiki_news, col_name = 'prompt_text')","metadata":{"execution":{"iopub.status.busy":"2023-10-11T23:22:15.408382Z","iopub.execute_input":"2023-10-11T23:22:15.408752Z","iopub.status.idle":"2023-10-11T23:23:44.781603Z","shell.execute_reply.started":"2023-10-11T23:22:15.408718Z","shell.execute_reply":"2023-10-11T23:23:44.780569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-11T23:23:50.042857Z","iopub.execute_input":"2023-10-11T23:23:50.043180Z","iopub.status.idle":"2023-10-11T23:23:50.060165Z","shell.execute_reply.started":"2023-10-11T23:23:50.043153Z","shell.execute_reply":"2023-10-11T23:23:50.059103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-11T23:23:50.286777Z","iopub.execute_input":"2023-10-11T23:23:50.287026Z","iopub.status.idle":"2023-10-11T23:23:50.299806Z","shell.execute_reply.started":"2023-10-11T23:23:50.287004Z","shell.execute_reply":"2023-10-11T23:23:50.298810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-11T23:23:50.409613Z","iopub.execute_input":"2023-10-11T23:23:50.410420Z","iopub.status.idle":"2023-10-11T23:23:50.422601Z","shell.execute_reply.started":"2023-10-11T23:23:50.410387Z","shell.execute_reply":"2023-10-11T23:23:50.421403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-11T23:23:50.635794Z","iopub.execute_input":"2023-10-11T23:23:50.636606Z","iopub.status.idle":"2023-10-11T23:23:50.646520Z","shell.execute_reply.started":"2023-10-11T23:23:50.636573Z","shell.execute_reply":"2023-10-11T23:23:50.645435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Identify and Handle Outlier Data ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef identify_outliers(data, feature):\n    \"\"\"\n    Identify outliers in a given feature based on the IQR method.\n    \n    Parameters:\n        data (pd.Series): The feature data.\n        \n    Returns:\n        np.ndarray: Boolean array indicating whether each sample is an outlier.\n    \"\"\"\n    Q1 = data.quantile(0.25)\n    Q3 = data.quantile(0.75)\n    IQR = Q3 - Q1\n    \n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    return (data < lower_bound) | (data > upper_bound), lower_bound, upper_bound\n\n\n# Identify outliers in the 'target' column\noutliers_content, lower_bound_cont, upper_bound_cont = identify_outliers(summaries_train['content'], 'content')\noutliers_wording, lower_bound_word, upper_bound_word = identify_outliers(summaries_train['wording'], 'wording')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_train[outliers_content==True]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_train[outliers_wording==True]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cap_floor_outliers(data, lower_bound, upper_bound):\n    \"\"\"\n    Cap and floor outliers to the upper and lower bounds, respectively.\n    \n    Parameters:\n        data (pd.Series): The feature data.\n        lower_bound (float): The lower bound to cap the data.\n        upper_bound (float): The upper bound to cap the data.\n        \n    Returns:\n        pd.Series: The feature data with capped and floored outliers.\n    \"\"\"\n    return np.where(data > upper_bound, upper_bound,\n                    np.where(data < lower_bound, lower_bound, data))\n\n# Cap and floor outliers in the 'target' column\nsummaries_train['content'] = cap_floor_outliers(summaries_train['content'], lower_bound_cont, upper_bound_cont)\nsummaries_train['wording'] = cap_floor_outliers(summaries_train['wording'], lower_bound_word, upper_bound_word)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the Data in to Training and Validation Sets","metadata":{}},{"cell_type":"code","source":"def custom_train_validation_split(summaries, prompts, test_size=0.2, random_state=None):\n    \n    \"\"\"\n    Custom function to perform train-validation split ensuring that\n    the same prompt IDs are in both training and validation sets.\n\n    Parameters:\n    - summaries: DataFrame containing summaries and associated prompt_ids\n    - prompts: DataFrame containing prompts and associated prompt_ids\n    - test_size: Proportion of the dataset to be used as the validation set\n    - random_state: Random seed for reproducibility\n\n    Returns:\n    - train_summaries: Training set containing summaries\n    - validation_summaries: Validation set containing summaries\n    - train_prompts: Training set containing prompts\n    - validation_prompts: Validation set containing prompts\n    \"\"\"\n    \n    # Extract unique prompt IDs\n    unique_prompt_ids = summaries['prompt_id'].unique()\n\n    # Split the unique prompt IDs into training and validation sets\n    train_ids, validation_ids = train_test_split(unique_prompt_ids, test_size=test_size, random_state=random_state)\n\n    # Use these IDs to filter the original summaries and prompts DataFrames\n    train_summaries = summaries[summaries['prompt_id'].isin(train_ids)]\n    validation_summaries = summaries[summaries['prompt_id'].isin(validation_ids)]\n    train_prompts = prompts[prompts['prompt_id'].isin(train_ids)]\n    validation_prompts = prompts[prompts['prompt_id'].isin(validation_ids)]\n\n    return train_summaries, validation_summaries, train_prompts, validation_prompts\n\n\n\n# Usage \ntrain_summaries, validation_summaries, train_prompts, validation_prompts = custom_train_validation_split(summaries_train, prompts_train, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T23:23:57.151375Z","iopub.execute_input":"2023-10-11T23:23:57.151978Z","iopub.status.idle":"2023-10-11T23:23:57.165515Z","shell.execute_reply.started":"2023-10-11T23:23:57.151944Z","shell.execute_reply":"2023-10-11T23:23:57.164597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings('ignore', category=UserWarning, module='scipy')\n\n# Initialize stopwords\nstop_words = set(stopwords.words('english'))\n\nfrom math import ceil\n\ndef preprocess(texts, batch_size=1000):\n    \"\"\"\n    Preprocesses a given series of texts by tokenizing, converting to lowercase,\n    and removing stopwords and non-alphanumeric words. Uses batching to avoid IOPub message rate exceeded error.\n    \n    Args:\n        texts (pd.Series): Series of texts to preprocess.\n        batch_size (int): Number of documents to process in each batch.\n        \n    Returns:\n        pd.Series: Preprocessed texts.\n    \"\"\"\n    # Calculate the number of batches\n    num_batches = ceil(len(texts) / batch_size)\n    \n    # Initialize an empty list to hold the preprocessed texts\n    preprocessed_texts = []\n    \n    for i in tqdm(range(num_batches), desc='Batch Processing', leave=False):\n        # Extract the texts for the current batch\n        start_idx = i * batch_size\n        end_idx = (i + 1) * batch_size\n        batch_texts = texts.iloc[start_idx:end_idx]\n        \n        # Preprocess the texts in the current batch\n        preprocessed_batch = batch_texts.apply(lambda x: [word for word in word_tokenize(x.lower()) if word.isalnum() and word not in stop_words])\n        \n        # Append the preprocessed texts to the list\n        preprocessed_texts.extend(preprocessed_batch)\n        \n    return pd.Series(preprocessed_texts)\n\n\ndef topic_modeling(texts, num_topics=5):\n    \"\"\"\n    Perform topic modeling using LDA.\n    \"\"\"\n    dictionary = corpora.Dictionary(texts)\n    corpus = [dictionary.doc2bow(text) for text in tqdm(texts, desc='Creating corpus', leave=False)]\n    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n    lda_display = gen.prepare(lda_model, corpus, dictionary)\n    return lda_display\n\ndef sentiment_analysis(texts):\n    \"\"\"\n    Perform sentiment analysis on the text using TextBlob.\n    \"\"\"\n    return texts.apply(lambda x: TextBlob(x).sentiment.polarity)\n\ndef text_complexity(texts):\n    \"\"\"\n    Calculate the Flesch Reading Ease score of the text using textstat.\n    \"\"\"\n    return texts.apply(lambda x: flesch_reading_ease(x))\n\n\n# Preprocess the text in the summaries and prompts\nprint(\"Preprocessing text in summaries...\")\nsummaries_processed = preprocess(summaries_train['text'])\nprint(\"Preprocessing text in prompts...\")\nprompts_processed = preprocess(prompts_train['prompt_text'])\n\n# Perform topic modeling on both summaries and prompts\nprint(\"Performing topic modeling on summaries...\")\nlda_display_summaries = topic_modeling(summaries_processed)\nprint(\"Performing topic modeling on prompts...\")\nlda_display_prompts = topic_modeling(prompts_processed)\n\n# Perform sentiment analysis on summaries and prompts\nprint(\"Performing sentiment analysis on summaries...\")\nsummaries_train['sentiment'] = sentiment_analysis(summaries_train['text'])\nprint(\"Performing sentiment analysis on prompts...\")\nprompts_train['sentiment'] = sentiment_analysis(prompts_train['prompt_text'])\n\n# Calculate text complexity for summaries and prompts\nprint(\"Calculating text complexity for summaries...\")\nsummaries_train['flesch_score'] = text_complexity(summaries_train['text'])\nprint(\"Calculating text complexity for prompts...\")\nprompts_train['flesch_score'] = text_complexity(prompts_train['prompt_text'])\n\nprint('\\n Preparing Visualizations.....')\n\n# Visualize sentiment and text complexity using histograms\nsns.histplot(summaries_train['sentiment']).set_title('Sentiment Distribution in Summaries')\nplt.show()\nsns.histplot(prompts_train['sentiment']).set_title('Sentiment Distribution in Prompts')\nplt.show()\nsns.histplot(summaries_train['flesch_score']).set_title('Flesch Reading Ease Score Distribution in Summaries')\nplt.show()\nsns.histplot(prompts_train['flesch_score']).set_title('Flesch Reading Ease Score Distribution in Prompts')\nplt.show()\n\n# Named Entity Recognition (Limiting to first 100 rows for demonstration)\nnlp = spacy.load(\"en_core_web_sm\")\ntexts_summaries = ' '.join(summaries_train['text'][:100])\ntexts_prompts = ' '.join(prompts_train['prompt_text'][:100])\ndoc_summaries = nlp(texts_summaries)\ndoc_prompts = nlp(texts_prompts)\n\n# Count the frequencies of named entity types in summaries and prompts\nentity_freq_summaries = Counter([ent.label_ for ent in doc_summaries.ents])\nentity_freq_prompts = Counter([ent.label_ for ent in doc_prompts.ents])\n\n# Visualize named entity frequencies\nplt.figure(figsize=(10, 6))\nplt.bar(entity_freq_summaries.keys(), entity_freq_summaries.values())\nplt.title('Named Entity Frequency in Summaries')\nplt.xticks(rotation=45)\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.bar(entity_freq_prompts.keys(), entity_freq_prompts.values())\nplt.title('Named Entity Frequency in Prompts')\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Process Test Data","metadata":{}},{"cell_type":"code","source":"def extract_features(texts, tfidf_vectorizer=None):\n    \n    \"\"\"\n    Extracts TF-IDF features from a list of texts.\n    \n    Parameters:\n    - texts (list): A list of strings containing the text to be processed.\n    - tfidf_vectorizer (TfidfVectorizer, optional): A pre-fitted TfidfVectorizer. If None, a new one will be fitted.\n    \n    Returns:\n    - array: The TF-IDF features in dense array format.\n    - TfidfVectorizer: The fitted or provided TfidfVectorizer instance.\n    \"\"\"\n    \n    # Initialize the TfidfVectorizer if not provided\n    if tfidf_vectorizer is None:\n        tfidf_vectorizer = TfidfVectorizer()\n        tfidf_features = tfidf_vectorizer.fit_transform(texts)\n    else:\n        # Transform texts using the provided TfidfVectorizer\n        tfidf_features = tfidf_vectorizer.transform(texts)\n        \n    # Convert the sparse array to a dense array\n    return tfidf_features.toarray(), tfidf_vectorizer\n\n\ndef preprocess_text(text):\n    \n    \"\"\"\n    Preprocesses a single text string.\n    \n    Parameters:\n    - text (str): The text to preprocess.\n    \n    Returns:\n    - str: The preprocessed text.\n    \"\"\"\n    \n    # Tokenize the text into words\n    tokens = nltk.word_tokenize(text)\n    \n    # Convert all tokens to lowercase\n    tokens = [token.lower() for token in tokens]\n    \n    # Remove non-alphanumeric tokens\n    tokens = [token for token in tokens if token.isalnum()]\n    \n    # Remove stop words\n#     stop_words = set(stopwords.words('english'))\n#     tokens = [token for token in tokens if token not in stop_words]\n    \n    # Reconstruct the text from the processed tokens\n    preprocessed_text = ' '.join(tokens)\n    \n    return preprocessed_text\n\n\ndef preprocess_data(summaries, prompts, tfidf_vectorizer=None):\n    \n    \"\"\"\n    Preprocesses and computes features for a dataset with text summaries and prompts.\n    \n    Parameters:\n    - summaries (DataFrame): DataFrame containing the text summaries.\n    - prompts (DataFrame): DataFrame containing the text prompts.\n    - tfidf_vectorizer (TfidfVectorizer, optional): A pre-fitted TfidfVectorizer.\n    \n    Returns:\n    - DataFrame: The preprocessed and feature-engineered DataFrame.\n    \"\"\"\n    \n    # Merge the summaries and prompts DataFrames on the 'prompt_id' column\n    print('Merging Prompts and Summaries.......')\n    \n    merged_data = summaries.merge(prompts, on='prompt_id', how='left', suffixes=('_summaries', '_prompts'))\n#     merged_data = summaries.merge(prompts, on='prompt_id', how='left')\n    merged_data['combined_text'] = merged_data['treated_question_summaries'] + \" \" + merged_data['treated_question_prompts']\n\n    # Apply text preprocessing\n    merged_data['treated_question_summaries'] = merged_data['treated_question_summaries'].apply(preprocess_text)\n\n    # Extract TF-IDF features\n#     tfidf_features, tfidf_vectorizer = extract_features(merged_data['preprocessed_text'], tfidf_vectorizer=tfidf_vectorizer)\n\n    stop_words = set(stopwords.words('english'))\n    print('Generating Text Based Features.......')\n    # Compute word count, sentence count, text length, and stopword count\n    merged_data['word_count'] = merged_data['treated_question_summaries'].apply(lambda x: len(word_tokenize(x)))\n    merged_data['sentence_count'] = merged_data['treated_question_summaries'].apply(lambda x: len(sent_tokenize(x)))\n    merged_data['len_text'] = merged_data['treated_question_summaries'].str.len()\n    merged_data['stop_count'] = merged_data['treated_question_summaries'].apply(lambda x: len([word for word in word_tokenize(x) if word in stop_words]))\n    import string\n    merged_data['punct_count'] = merged_data['treated_question_summaries'].apply(lambda x: len([char for char in x if char in string.punctuation]))\n    merged_data['capital_count'] = merged_data['treated_question_summaries'].apply(lambda x: len([word for word in word_tokenize(x) if word.isupper()]))\n    from nltk import pos_tag\n    merged_data['noun_count'] = merged_data['treated_question_summaries'].apply(lambda x: len([word for word, pos in pos_tag(word_tokenize(x)) if pos.startswith('NN')]))\n    import nltk\n    from nltk import ne_chunk\n    merged_data['ne_count'] = merged_data['treated_question_summaries'].apply(lambda x: len([chunk for chunk in ne_chunk(pos_tag(word_tokenize(x))) if hasattr(chunk, 'label')]))\n    merged_data['avg_word_len'] = merged_data['treated_question_summaries'].apply(lambda x: sum(len(word) for word in word_tokenize(x)) / len(word_tokenize(x)) if len(word_tokenize(x)) > 0 else 0)\n    merged_data['lex_div'] = merged_data['treated_question_summaries'].apply(lambda x: len(set(word_tokenize(x))) / len(word_tokenize(x)) if len(word_tokenize(x)) > 0 else 0)\n    from textblob import TextBlob\n    merged_data['polarity'] = merged_data['treated_question_summaries'].apply(lambda x: TextBlob(x).sentiment.polarity)\n    merged_data['subjectivity'] = merged_data['treated_question_summaries'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n    from collections import Counter\n    merged_data['most_common_word_count'] = merged_data['treated_question_summaries'].apply(lambda x: Counter(word_tokenize(x)).most_common(1)[0][1] if len(word_tokenize(x)) > 0 else 0)\n\n    \n    # Calculate cosine similarity between the text and its corresponding prompt\n    print('Computing Cosine Similarity.......')\n    merged_data = compute_cosine_similarity(merged_data, 'treated_question_summaries', 'treated_question_prompts')\n    \n    return merged_data\n\n\ndef compute_cosine_similarity(df, text_col, content_col):\n    \n    \"\"\"\n    Computes the cosine similarity between two columns of text in a DataFrame.\n    \n    Parameters:\n    - df (DataFrame): The DataFrame containing the texts.\n    - text_col (str): The name of the column containing the first set of texts.\n    - content_col (str): The name of the column containing the second set of texts.\n    \n    Returns:\n    - DataFrame: The DataFrame with an additional column for the computed cosine similarity.\n    \"\"\"\n    \n    # Combine texts from both columns to fit the TF-IDF vectorizer\n    all_texts = df[text_col].tolist() + df[content_col].tolist()\n    \n    # Fit the TF-IDF vectorizer on the combined corpus\n    vectorizer = TfidfVectorizer()\n    vectorizer.fit(all_texts)\n    \n    # Generate TF-IDF vectors for both columns\n    text_tfidf = vectorizer.transform(df[text_col])\n    content_tfidf = vectorizer.transform(df[content_col])\n    \n    # Compute cosine similarity for each pair of text and content\n    cosine_sim_values = [cosine_similarity(text_tfidf[i], content_tfidf[i])[0][0] for i in range(len(df))]\n    \n    # Add the computed cosine similarity values to the DataFrame\n    df['cos_sim'] = cosine_sim_values\n    \n    return df\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T23:24:05.898204Z","iopub.execute_input":"2023-10-11T23:24:05.898572Z","iopub.status.idle":"2023-10-11T23:24:05.916373Z","shell.execute_reply.started":"2023-10-11T23:24:05.898540Z","shell.execute_reply":"2023-10-11T23:24:05.915424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For training data\nmerged_train_data = preprocess_data(train_summaries, train_prompts)\n\nmerged_validation_data = preprocess_data(validation_summaries, validation_prompts)\n\n# For test data\nmerged_test_data = preprocess_data(summaries_test, prompts_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T23:24:06.397296Z","iopub.execute_input":"2023-10-11T23:24:06.397657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_data = preprocess_data(summaries_train, prompts_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train_data.to_csv('merged_train_data.csv')\nmerged_validation_data.to_csv('merged_validation_data.csv')\nmerged_test_data.to_csv('merged_test_data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train_data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_cols = ['prompt_id', 'text', 'lowered_question_summaries', 'cleaned_text_summaries',\n       'treated_question_summaries', 'prompt_question', 'prompt_title',\n       'prompt_text', 'lowered_question_prompts', 'cleaned_text_prompts',\n       'treated_question_prompts', 'combined_text']\n\nmerged_train_data.drop(columns=drop_cols, inplace= True)\nmerged_validation_data.drop(columns=drop_cols, inplace= True)\nmerged_test_data.drop(columns=drop_cols, inplace= True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_data.drop(columns=drop_cols, inplace= True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train_data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_cols = []\n\nfor col in merged_train_data.columns:\n    if (col != 'student_id') and (col != 'content') and (col != 'wording'):\n        feature_cols.append(col) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select relevant columns (replace with actual column names)\n\ntarget_labels = merged_train_data[['content', 'wording']]\n\nvalidation_labels = merged_validation_data[['content', 'wording']]\n\ntrain_features = merged_train_data[feature_cols].values\n\nval_features = merged_validation_data[feature_cols].values\n\ntest_features = merged_test_data[feature_cols].values\n\n\n# Standardize the features if needed\n\nscaler = StandardScaler()\n\n# target_scaler = MinMaxScaler(feature_range=(-1, 1))\n\ntarget_scaler = MinMaxScaler()\n\ntrain_features = scaler.fit_transform(train_features)\n\nval_features = scaler.transform(val_features)\n\ntest_features = scaler.transform(test_features)\n\ntarget_labels_scaled = target_scaler.fit_transform(target_labels)\n\nvalidation_labels_scaled = target_scaler.transform(validation_labels)\n\n# Optionally, convert back to DataFrame\ntarget_labels = pd.DataFrame(target_labels_scaled, columns=['content', 'wording'], index=target_labels.index)\nvalidation_labels = pd.DataFrame(validation_labels_scaled, columns=['content', 'wording'], index=validation_labels.index)\n\nimport pickle\n\n# Save the scaler and other processed data\nwith open('target_scaler.pkl', 'wb') as f:\n    pickle.dump(target_scaler, f)\n\nwith open('scaler.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n    \n# Combine TF-IDF features with the selected features\n\n# combined_train_features = np.hstack((tfidf_train_features, train_features))\n\n# combined_test_features = np.hstack((tfidf_test_features, test_features))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_reg = tf.data.Dataset.from_tensor_slices((train_features, target_labels)).shuffle(len(train_features)).batch(32)\nval_reg = tf.data.Dataset.from_tensor_slices((val_features, validation_labels)).batch(32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## num_words:\n\nPurpose: \n\n    This parameter sets the maximum number of words to keep, based on word frequency. Only the most common num_words will be kept. If set to None, all words will be kept.\n\nHow to Choose:\n\n    You can start by examining the distribution of word frequencies in your text corpus. This can give you an idea of how many unique words are in your data, and how frequently they occur.\n    \n    If computational resources are limited, or if you want to avoid overfitting, you might set this value to limit the vocabulary size.\n\n    If the dataset is large and diverse, keeping all words (None) might be feasible and beneficial. But if the dataset is small, limiting the vocabulary might prevent overfitting.\n\n## Here's an example code snippet to plot word frequency using Python:","metadata":{}},{"cell_type":"code","source":"# Assuming 'train_data' contains the preprocessed training text in a column called 'preprocessed_text'\nwords = ' '.join(train_data['preprocessed_text']).split()\nword_freq = Counter(words)\n\n# Sort by frequency\nsorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n\n# Plot\nplt.figure(figsize=(15, 7))\nplt.bar([x[0] for x in sorted_word_freq[:50]], [x[1] for x in sorted_word_freq[:50]])\nplt.xticks(rotation=45)\nplt.xlabel(\"Words\")\nplt.ylabel(\"Frequency\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the Loss Function (MCrmse)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ndef mcrmse_loss(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n    \"\"\"\n    Compute the Mean Columnwise Root Mean Squared Error (MCRMSE).\n    \n    Parameters:\n    - y_true : tf.Tensor\n        Ground truth values.\n        \n    - y_pred : tf.Tensor\n        Predicted values.\n    \n    Returns:\n    - mcrmse : tf.Tensor\n        The computed MCRMSE value.\n    \"\"\"\n    columnwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=0)\n    columnwise_rmse = tf.sqrt(columnwise_mse)\n    mcrmse = tf.reduce_mean(columnwise_rmse)\n    return mcrmse\n\ndef weighted_mcrmse_loss_factory(weights):\n    \"\"\"\n    Factory function for generating a custom loss function that combines different losses.\n    \n    Parameters:\n    - weights: List of weights for each loss component\n    \n    Returns:\n    - weighted_mcrmse_loss: A function for computing the weighted loss\n    \"\"\"\n    def weighted_mcrmse_loss(y_true, y_pred):\n        \"\"\"\n        Compute the weighted Mean Columnwise Root Mean Squared Error (MCRMSE).\n        \n        Parameters:\n        - y_true: True labels\n        - y_pred: Predicted labels\n        \n        Returns:\n        - loss: The weighted loss\n        \"\"\"\n        # Calculate columnwise MSE and RMSE\n        columnwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=0)\n        columnwise_rmse = tf.sqrt(columnwise_mse)\n        \n        # Apply weights to the columnwise RMSE\n        weighted_rmse = tf.multiply(columnwise_rmse, weights)\n        \n        # Take the mean across all columns to get the final loss\n        loss = tf.reduce_mean(weighted_rmse)\n        \n        return loss\n    \n    return weighted_mcrmse_loss\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feedforward Networks","metadata":{}},{"cell_type":"code","source":"print(f'Training Target Mean: \\n {np.mean(target_labels)}')\nprint(f'Validation Target Mean: \\n {np.mean(validation_labels)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_features), len(target_labels))\nprint(len(val_features), len(validation_labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import schedules, AdamW, RMSprop\n\ndef build_regression_model(hp):\n    \n    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2)  # number of hidden layers\n    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=256)  # neurons in each hidden layer\n    l2_reg = hp.Float(\"l2_reg\", min_value=1e-6, max_value=1e-2, sampling=\"log\")  # L2 regularization\n    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")  # learning rate\n    \n    optimizer_choice = hp.Choice(\"optimizer_choice\", ['adam', 'sgd', 'RMSprop', 'Adagrad', 'Adadelta', 'Nadam', 'Ftrl', 'L-BFGS'])\n    \n    # Learning rate schedulers\n    lr_schedule = schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n                                             decay_steps=10000,\n                                             decay_rate=0.9)\n    \n    if optimizer_choice == 'adam':\n        optimizer = AdamW(learning_rate=lr_schedule)\n    elif optimizer_choice == 'sgd':\n        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n    elif optimizer_choice == 'RMSprop':\n        optimizer = RMSprop(learning_rate=lr_schedule)\n    elif optimizer_choice == 'Adagrad':\n        optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr_schedule)\n    elif optimizer_choice == 'Adadelta':\n        optimizer = tf.keras.optimizers.Adadelta(learning_rate=lr_schedule)\n    elif optimizer_choice == 'Nadam':\n        optimizer = tf.keras.optimizers.Nadam(learning_rate=lr_schedule)\n    else:\n        # Ftrl will be the default optimizer\n        optimizer = tf.keras.optimizers.Ftrl(learning_rate=lr_schedule)\n\n    \n    model = tf.keras.models.Sequential()\n    \n    model.add(tf.keras.layers.Dense(hp.Int('input_units', min_value=32, max_value=512, step=32), \n                                    activation='relu', input_shape=(train_features.shape[1],)))  # Assuming X_train is pre-defined\n    \n    dropout_rate = hp.Float(\"dropout_rate\", min_value=0.0, max_value=0.5, step=0.05)  # dropout rate\n    \n    # Adding Batch Normalization and dropout for each hidden layer\n    for _ in range(n_hidden):\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dense(n_neurons, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_reg)))\n        model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n    \n    model.add(tf.keras.layers.Dense(2))  # Output layer\n    \n    my_weighted_loss = weighted_mcrmse_loss_factory([.4, .6])\n    \n    model.compile(optimizer=optimizer, loss=my_weighted_loss)\n\n\n    # Learning rate reduction callback\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5)\n    \n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_regression_model2(hp):\n    n_hidden = hp.Int(\n        \"n_hidden\", min_value=0, max_value=8, default=2\n    )  # number of hidden layers\n\n    n_neurons = hp.Int(\n        \"n_neurons\", min_value=16, max_value=256\n    )  # number of neurons in each hidden layer\n\n\n    l2_reg = hp.Float(\"l2_reg\", min_value=1e-6, max_value=1e-2, sampling=\"log\")\n\n    learning_rate = hp.Float(\n        \"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")  # learning rate\n\n    optimizer_choice = hp.Choice(\"optimizer_choice\", ['adam', 'sgd', 'RMSprop', 'Adagrad', 'Adadelta', 'Nadam', 'Ftrl', 'L-BFGS'])\n    \n    # Learning rate schedulers\n    lr_schedule = schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n                                             decay_steps=10000,\n                                             decay_rate=0.9)\n    \n    if optimizer_choice == 'adam':\n        optimizer = AdamW(learning_rate=lr_schedule)\n    elif optimizer_choice == 'sgd':\n        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n    elif optimizer_choice == 'RMSprop':\n        optimizer = RMSprop(learning_rate=lr_schedule)\n    elif optimizer_choice == 'Adagrad':\n        optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr_schedule)\n    elif optimizer_choice == 'Adadelta':\n        optimizer = tf.keras.optimizers.Adadelta(learning_rate=lr_schedule)\n    elif optimizer_choice == 'Nadam':\n        optimizer = tf.keras.optimizers.Nadam(learning_rate=lr_schedule)\n    elif optimizer_choice == 'Ftrl':\n        optimizer = tf.keras.optimizers.Ftrl(learning_rate=lr_schedule)\n    # L-BFGS is a bit different and may require separate handling, left out here for simplicity\n    \n    model = tf.keras.models.Sequential()\n\n    model.add(tf.keras.layers.Dense(hp.Int('input_units', min_value=32, max_value=512, step=32), \n                                    activation='relu', input_shape=(train_features.shape[1],)))\n\n    model.add(tf.keras.layers.BatchNormalization())  # Batch Normalization\n\n    for _ in range(n_hidden):\n        model.add(tf.keras.layers.Dense(n_neurons, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_reg)))\n        model.add(tf.keras.layers.Dropout(rate=hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.05)))  # Dropout\n\n    model.add(tf.keras.layers.Dense(2))  # Assuming you want to stick with a linear activation\n\n    my_weighted_loss = weighted_mcrmse_loss_factory([0.4, 0.6])\n    \n    model.compile(optimizer=optimizer, loss=mcrmse_loss)    \n    \n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = best_complex_model.predict(val_features)\npredictions = target_scaler.inverse_transform(predictions)\npredictions\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true = validation_labels\ntrue = target_scaler.inverse_transform(true)\n# true['c_pred'] = predictions[:,0]\n# true['w_pred'] = predictions[:,1]\ntrue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(true, predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_diff = np.mean(true - predictions)\nmean_diff","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = ModelCheckpoint('best_standard_model_epoch.h5', save_best_only=True, monitor='val_loss', mode='min')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\ncall_backs = [model_checkpoint, early_stopping]\n\ntuner_2 = kt.BayesianOptimization(\n    build_regression_model2,\n    objective='val_loss',\n    max_trials=8,\n    num_initial_points= 2,\n    seed=1,\n    overwrite=True,\n    directory='tuner_2',\n    project_name='standard'\n)\n\ntuner_2.search(train_reg, epochs=1000, validation_data=val_reg, callbacks = call_backs)\n\nbest_standard_model = tuner_2.get_best_models(num_models=1)[0]\n\n# best_bayes_model.save('best_model.h5')\n\nbest_standard_model.save('best_standard_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = best_standard_model.predict(test_features)\npredictions = target_scaler.inverse_transform(predictions)\npredictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = best_standard_model.predict(val_features)\npredictions = target_scaler.inverse_transform(predictions)\npredictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true = validation_labels\ntrue = target_scaler.inverse_transform(true)\ntrue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_diff = np.mean(true - predictions)\nprint(f' Mean Difference {mean_diff} \\n')\n\nplt.scatter(true, predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('scaler.pkl', 'wb') as f:\n    pickle.dump(scaler, f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Recreate the best model using the best hyperparameters found\nbest_hyperparameters = tuner_2.get_best_hyperparameters()[0]\nfinal_model = build_regression_model2(best_hyperparameters)\n\n# Callbacks for full data\nmodel_checkpoint_full = ModelCheckpoint('best_standard_model_epoch.h5', \n                                         save_best_only=True, monitor='val_loss', mode='min')\n\ncall_backs_full = [model_checkpoint_full]\n\n# Train the model on the full data\nfinal_model.fit(merged_tr, full_train_labels, epochs=1000, callbacks=call_backs_full)\n\n# Save the model\nfinal_model.save('best_model_full_data.h5')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Best Models","metadata":{}},{"cell_type":"code","source":"loss, mse, mae = best_model.evaluate(X_val, y_val)\n\n\nprint(\"Loss: \", loss)\nprint(\"MSE: \", mse)\nprint(\"MAE: \", mae)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nloss = history.history['loss']\n# val_loss = history.history['val_loss']\nmse = history.history['mse']\n# val_mse = history.history['val_mse']\nmae = history.history['mae']\n# val_mae = history.history['val_mae']\n\n\n\nfig , ax = plt.subplots(1,3, figsize=(20,5))\n\nax[0].plot(loss, label='train', color='purple', linewidth=2.0, linestyle = '-')   # specify color and line width\n# ax[0].plot(val_loss, label='validation', color='blue', linewidth=2.0, linestyle = '-')  # specify color and line width\nax[0].set_title('loss')\nax[0].set_xlabel('Epochs')   # set x label\nax[0].set_ylabel('Loss')   # set y label\nax[0].legend()\n\nax[1].plot(mse, label='train', color='purple', linewidth=2.0, linestyle = '-')   # specify color and line width\n# ax[1].plot(val_mse, label='validation', color='blue', linewidth=2.0, linestyle = '-')  # specify color and line width\nax[1].set_title('mse')\nax[1].set_xlabel('Epochs')   # set x label\nax[1].set_ylabel('MSE')   # set y label\nax[1].legend()\n\nax[2].plot(mae, label='train', color='purple', linewidth=2.0, linestyle = '-')   # specify color and line width\n# ax[2].plot(val_mae, label='validation', color='blue', linewidth=2.0, linestyle = '-')  # specify color and line width\nax[2].set_title('mae')\nax[2].set_xlabel('Epochs')   # set x label\nax[2].set_ylabel('MAE')   # set y label\nax[2].legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model.save(\"final_model.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model = tf.keras.models.load_model(\"final_model.h5\", custom_objects=custom_objects)\npredictions = final_model.predict(X_val)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(predictions-y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}